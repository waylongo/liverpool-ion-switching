{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:23.283334Z",
     "start_time": "2020-05-04T22:31:22.003450Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, gc, random\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "import tensorflow_addons as tfa\n",
    "from tf_nn_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# set gpu memory growth\n",
    "gpus= tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:23.286129Z",
     "start_time": "2020-05-04T22:31:23.284344Z"
    }
   },
   "outputs": [],
   "source": [
    "# configurations and main hyperparammeters\n",
    "nn_epochs = 100\n",
    "nn_batch_size = 16\n",
    "class_num = 6\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:23.907967Z",
     "start_time": "2020-05-04T22:31:23.287290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:(4500000, 17), test size:(2000000, 18)\n"
     ]
    }
   ],
   "source": [
    "df_train_raw = pd.read_pickle('../features/train_clean.pkl')\n",
    "df_test_raw = pd.read_pickle('../features/test_clean.pkl')\n",
    "TARGET = \"open_channels\"\n",
    "df_test_raw[TARGET] = 0\n",
    "\n",
    "# RFC features\n",
    "Y_train_proba = np.load(\"../features/Y_train_proba.npy\")\n",
    "Y_test_proba = np.load(\"../features/Y_test_proba.npy\")\n",
    "Y_train_proba = np.delete(Y_train_proba, list(range(3500000, 4000000)), 0)\n",
    "for i in range(11):\n",
    "    df_train_raw[f\"proba_{i}\"] = Y_train_proba[:, i]\n",
    "    df_test_raw[f\"proba_{i}\"] = Y_test_proba[:, i]\n",
    "\n",
    "print(f\"train size:{df_train_raw.shape}, test size:{df_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:30.800545Z",
     "start_time": "2020-05-04T22:31:23.908965Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature engineering here\n",
    "def fe(df, is_train):\n",
    "\n",
    "    df[\"group\"] = df[\"batch\"].astype(\"str\") + \"_\" + df[\"mini_batch\"].astype(\"str\")\n",
    "    \n",
    "    # shift features\n",
    "    for shift_val in range(1, 4):\n",
    "        group_on = \"batch\" if is_train else \"group\"\n",
    "        df[f'shift+{shift_val}'] = df.groupby([group_on])['signal'].shift(shift_val).fillna(0)\n",
    "        df[f'shift_{shift_val}'] = df.groupby([group_on])['signal'].shift(-shift_val).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df_train_raw = fe(df_train_raw, is_train=1)\n",
    "df_test_raw = fe(df_test_raw, is_train=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:30.819958Z",
     "start_time": "2020-05-04T22:31:30.801516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>signal</th>\n",
       "      <th>open_channels</th>\n",
       "      <th>local_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>mini_batch</th>\n",
       "      <th>proba_0</th>\n",
       "      <th>proba_1</th>\n",
       "      <th>proba_2</th>\n",
       "      <th>proba_3</th>\n",
       "      <th>proba_4</th>\n",
       "      <th>proba_5</th>\n",
       "      <th>proba_6</th>\n",
       "      <th>proba_7</th>\n",
       "      <th>proba_8</th>\n",
       "      <th>proba_9</th>\n",
       "      <th>proba_10</th>\n",
       "      <th>group</th>\n",
       "      <th>shift+1</th>\n",
       "      <th>shift_1</th>\n",
       "      <th>shift+2</th>\n",
       "      <th>shift_2</th>\n",
       "      <th>shift+3</th>\n",
       "      <th>shift_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>-2.7600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.966731</td>\n",
       "      <td>0.028343</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1_1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.8557</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.4074</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-3.1404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-2.8557</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996045</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1_1</td>\n",
       "      <td>-2.7600</td>\n",
       "      <td>-2.4074</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-3.1404</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-3.1525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0003</td>\n",
       "      <td>-2.4074</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976313</td>\n",
       "      <td>0.018989</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1_1</td>\n",
       "      <td>-2.8557</td>\n",
       "      <td>-3.1404</td>\n",
       "      <td>-2.7600</td>\n",
       "      <td>-3.1525</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.6418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0004</td>\n",
       "      <td>-3.1404</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996002</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1_1</td>\n",
       "      <td>-2.4074</td>\n",
       "      <td>-3.1525</td>\n",
       "      <td>-2.8557</td>\n",
       "      <td>-2.6418</td>\n",
       "      <td>-2.7600</td>\n",
       "      <td>-2.6993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>-3.1525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997465</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1_1</td>\n",
       "      <td>-3.1404</td>\n",
       "      <td>-2.6418</td>\n",
       "      <td>-2.4074</td>\n",
       "      <td>-2.6993</td>\n",
       "      <td>-2.8557</td>\n",
       "      <td>-2.5935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time  signal  open_channels  local_time  batch  mini_batch   proba_0  \\\n",
       "0  0.0001 -2.7600              0      0.0001      1           1  0.966731   \n",
       "1  0.0002 -2.8557              0      0.0002      1           1  0.996045   \n",
       "2  0.0003 -2.4074              0      0.0003      1           1  0.976313   \n",
       "3  0.0004 -3.1404              0      0.0004      1           1  0.996002   \n",
       "4  0.0005 -3.1525              0      0.0005      1           1  0.997465   \n",
       "\n",
       "    proba_1   proba_2   proba_3  proba_4  proba_5  proba_6  proba_7  proba_8  \\\n",
       "0  0.028343  0.004812  0.000114      0.0      0.0      0.0      0.0      0.0   \n",
       "1  0.003466  0.000426  0.000063      0.0      0.0      0.0      0.0      0.0   \n",
       "2  0.018989  0.004677  0.000021      0.0      0.0      0.0      0.0      0.0   \n",
       "3  0.003625  0.000326  0.000046      0.0      0.0      0.0      0.0      0.0   \n",
       "4  0.002335  0.000158  0.000042      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   proba_9  proba_10 group  shift+1  shift_1  shift+2  shift_2  shift+3  \\\n",
       "0      0.0       0.0   1_1   0.0000  -2.8557   0.0000  -2.4074   0.0000   \n",
       "1      0.0       0.0   1_1  -2.7600  -2.4074   0.0000  -3.1404   0.0000   \n",
       "2      0.0       0.0   1_1  -2.8557  -3.1404  -2.7600  -3.1525   0.0000   \n",
       "3      0.0       0.0   1_1  -2.4074  -3.1525  -2.8557  -2.6418  -2.7600   \n",
       "4      0.0       0.0   1_1  -3.1404  -2.6418  -2.4074  -2.6993  -2.8557   \n",
       "\n",
       "   shift_3  \n",
       "0  -3.1404  \n",
       "1  -3.1525  \n",
       "2  -2.6418  \n",
       "3  -2.6993  \n",
       "4  -2.5935  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:30.824085Z",
     "start_time": "2020-05-04T22:31:30.820943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used columns is ['signal', 'proba_0', 'proba_1', 'proba_2', 'proba_3', 'proba_4', 'proba_5', 'proba_6', 'proba_7', 'proba_8', 'proba_9', 'proba_10', 'shift+1', 'shift_1', 'shift+2', 'shift_2', 'shift+3', 'shift_3']\n"
     ]
    }
   ],
   "source": [
    "use_cols = [\n",
    "    col for col in df_train_raw.columns if col not in\n",
    "    [\"time\", \"local_time\", \"open_channels\", \"batch\", \"mini_batch\", \"group\"]\n",
    "]\n",
    "print(\"Used columns is\", use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:30.833453Z",
     "start_time": "2020-05-04T22:31:30.825026Z"
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 500\n",
    "def chop_seq(df_batch_i, is_train):\n",
    "\n",
    "    df_batch_i_features = []\n",
    "    df_batch_i_y = []\n",
    "    df_batch_i_group = []\n",
    "    \n",
    "    WHOLE_LEN = 5e5 if is_train else 1e5\n",
    "    \n",
    "    for i in range(int(WHOLE_LEN/SEQ_LEN)):\n",
    "\n",
    "        # (SEQ_LEN, 5)\n",
    "        tmp = df_batch_i[(SEQ_LEN * i):(SEQ_LEN * (i + 1))]\n",
    "        df_batch_i_features.append(tmp[use_cols].values)\n",
    "        df_batch_i_y.append(tmp[TARGET].values)\n",
    "        df_batch_i_group.append(tmp[\"group\"].values)\n",
    "\n",
    "    return df_batch_i_features, df_batch_i_y, df_batch_i_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:34.042984Z",
     "start_time": "2020-05-04T22:31:30.834948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: (4000, 500, 18) (4000, 500)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "df_train = []\n",
    "df_train_y = []\n",
    "groups = []\n",
    "\n",
    "for batch_i in [1, 2, 6, 9]:\n",
    "    df_batch_i = df_train_raw[df_train_raw.batch == batch_i]\n",
    "    df_batch_i_features, df_batch_i_y, df_batch_i_group = chop_seq(df_batch_i, is_train=1)\n",
    "    df_train.append(df_batch_i_features)\n",
    "    df_train_y.append(df_batch_i_y)\n",
    "    groups.append(df_batch_i_group)\n",
    "\n",
    "df_train = np.array(df_train).reshape(\n",
    "    [-1, SEQ_LEN, np.array(df_train).shape[-1]])\n",
    "df_train_y = np.array(df_train_y).reshape([-1, SEQ_LEN])\n",
    "groups = np.array(groups).reshape([-1, SEQ_LEN])[:,0]\n",
    "\n",
    "print(\"TRAIN:\", df_train.shape, df_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:34.229977Z",
     "start_time": "2020-05-04T22:31:34.043968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: (200, 500, 18) (200, 500)\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "df_test = []\n",
    "df_test_y = []\n",
    "df_test_groups = []\n",
    "\n",
    "mini_batch_list = [[3,3]]\n",
    "for batch_i, mini_batch_i in mini_batch_list:\n",
    "    df_batch_i = df_test_raw[(df_test_raw.batch == batch_i) & (df_test_raw.mini_batch == mini_batch_i)]\n",
    "    df_batch_i_features, df_batch_i_y, df_test_batch_i_group = chop_seq(df_batch_i, is_train=0)\n",
    "    df_test.append(df_batch_i_features)\n",
    "    df_test_y.append(df_batch_i_y)\n",
    "    df_test_groups.append(df_test_batch_i_group)\n",
    "\n",
    "df_test = np.array(df_test).reshape(\n",
    "    [-1, SEQ_LEN, np.array(df_test).shape[-1]])\n",
    "df_test_y = np.array(df_test_y).reshape([-1, SEQ_LEN])\n",
    "df_test_groups = np.array(df_test_groups).reshape([-1, SEQ_LEN])[:,0]\n",
    "\n",
    "print(\"TEST:\", df_test.shape, df_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:34.240449Z",
     "start_time": "2020-05-04T22:31:34.231060Z"
    }
   },
   "outputs": [],
   "source": [
    "# model function (very important, you can try different arquitectures to get a better score. I believe that top public leaderboard is a 1D Conv + RNN style)\n",
    "def Classifier(shape_):\n",
    "    \n",
    "    def cbr(x, out_layer, kernel, stride, dilation):\n",
    "        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "    \n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters = filters,\n",
    "                   kernel_size = 1,\n",
    "                   padding = 'same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same', \n",
    "                              activation = 'tanh', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same',\n",
    "                              activation = 'sigmoid', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out, sigm_out])\n",
    "            x = Conv1D(filters = filters,\n",
    "                       kernel_size = 1,\n",
    "                       padding = 'same')(x)\n",
    "            res_x = Add()([res_x, x])\n",
    "        return res_x\n",
    "    \n",
    "    inp = Input(shape = (shape_))\n",
    "    x = cbr(inp, 64, 5, 1, 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 32, 3, 8)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 64, 3, 4)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 128, 3, 1)\n",
    "    x = cbr(x, 32, 5, 1, 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(class_num, activation = 'softmax', name = 'out')(x)\n",
    "    \n",
    "    model = models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    opt = Adam(lr = LR)\n",
    "    opt = tfa.optimizers.SWA(opt)\n",
    "    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:34.347320Z",
     "start_time": "2020-05-04T22:31:34.241392Z"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "oof_ = np.zeros([df_train.shape[0], df_train.shape[1], class_num])\n",
    "preds_ = np.zeros((df_test.shape[0] * df_test.shape[1], class_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:31:34.382866Z",
     "start_time": "2020-05-04T22:31:34.348213Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_y = pd.get_dummies(df_train_y.reshape([-1])).values.reshape([-1, SEQ_LEN, class_num])\n",
    "df_test_y = np.zeros([df_train_y.shape[0], df_train_y.shape[1], class_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:40:30.506616Z",
     "start_time": "2020-05-04T22:31:34.383839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training dataset shape is (3200, 500, 18)\n",
      "Our validation dataset shape is (800, 500, 18)\n",
      "(3200, 500, 18) (3200, 500, 6)\n",
      "Evaluate on ['1_5' '2_5' '6_5' '9_5']\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /home/ww6p9/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "3200/3200 - 13s - loss: 0.1395 - accuracy: 0.9658 - val_loss: 0.1029 - val_accuracy: 0.9869\n",
      "Epoch 2/100\n",
      "3200/3200 - 4s - loss: 0.0527 - accuracy: 0.9878 - val_loss: 0.0400 - val_accuracy: 0.9887\n",
      "Epoch 3/100\n",
      "3200/3200 - 4s - loss: 0.0572 - accuracy: 0.9862 - val_loss: 0.0413 - val_accuracy: 0.9886\n",
      "Epoch 4/100\n",
      "3200/3200 - 3s - loss: 0.0440 - accuracy: 0.9880 - val_loss: 0.0369 - val_accuracy: 0.9888\n",
      "Epoch 5/100\n",
      "3200/3200 - 3s - loss: 0.0414 - accuracy: 0.9881 - val_loss: 0.0348 - val_accuracy: 0.9889\n",
      "Epoch 6/100\n",
      "3200/3200 - 3s - loss: 0.0405 - accuracy: 0.9881 - val_loss: 0.0351 - val_accuracy: 0.9889\n",
      "Epoch 7/100\n",
      "3200/3200 - 4s - loss: 0.0398 - accuracy: 0.9882 - val_loss: 0.0339 - val_accuracy: 0.9890\n",
      "Epoch 8/100\n",
      "3200/3200 - 4s - loss: 0.0386 - accuracy: 0.9882 - val_loss: 0.0356 - val_accuracy: 0.9887\n",
      "Epoch 9/100\n",
      "3200/3200 - 3s - loss: 0.0392 - accuracy: 0.9882 - val_loss: 0.0354 - val_accuracy: 0.9889\n",
      "Epoch 10/100\n",
      "3200/3200 - 3s - loss: 0.0373 - accuracy: 0.9883 - val_loss: 0.0340 - val_accuracy: 0.9889\n",
      "Epoch 11/100\n",
      "3200/3200 - 4s - loss: 0.0349 - accuracy: 0.9885 - val_loss: 0.0332 - val_accuracy: 0.9891\n",
      "Epoch 12/100\n",
      "3200/3200 - 3s - loss: 0.0352 - accuracy: 0.9885 - val_loss: 0.0330 - val_accuracy: 0.9889\n",
      "Epoch 13/100\n",
      "3200/3200 - 3s - loss: 0.0348 - accuracy: 0.9885 - val_loss: 0.0327 - val_accuracy: 0.9890\n",
      "Epoch 14/100\n",
      "3200/3200 - 3s - loss: 0.0340 - accuracy: 0.9886 - val_loss: 0.0327 - val_accuracy: 0.9890\n",
      "Epoch 15/100\n",
      "3200/3200 - 4s - loss: 0.0338 - accuracy: 0.9886 - val_loss: 0.0326 - val_accuracy: 0.9890\n",
      "Epoch 16/100\n",
      "3200/3200 - 4s - loss: 0.0332 - accuracy: 0.9887 - val_loss: 0.0328 - val_accuracy: 0.9890\n",
      "Epoch 17/100\n",
      "3200/3200 - 4s - loss: 0.0326 - accuracy: 0.9889 - val_loss: 0.0329 - val_accuracy: 0.9890\n",
      "Epoch 18/100\n",
      "3200/3200 - 3s - loss: 0.0326 - accuracy: 0.9889 - val_loss: 0.0325 - val_accuracy: 0.9892\n",
      "Epoch 19/100\n",
      "3200/3200 - 4s - loss: 0.0320 - accuracy: 0.9890 - val_loss: 0.0327 - val_accuracy: 0.9891\n",
      "Epoch 20/100\n",
      "3200/3200 - 3s - loss: 0.0317 - accuracy: 0.9891 - val_loss: 0.0321 - val_accuracy: 0.9892\n",
      "Epoch 21/100\n",
      "3200/3200 - 4s - loss: 0.0301 - accuracy: 0.9894 - val_loss: 0.0318 - val_accuracy: 0.9893\n",
      "Epoch 22/100\n",
      "3200/3200 - 3s - loss: 0.0300 - accuracy: 0.9894 - val_loss: 0.0321 - val_accuracy: 0.9891\n",
      "Epoch 23/100\n",
      "3200/3200 - 3s - loss: 0.0297 - accuracy: 0.9895 - val_loss: 0.0321 - val_accuracy: 0.9892\n",
      "Epoch 24/100\n",
      "3200/3200 - 3s - loss: 0.0294 - accuracy: 0.9895 - val_loss: 0.0323 - val_accuracy: 0.9892\n",
      "Epoch 25/100\n",
      "3200/3200 - 3s - loss: 0.0293 - accuracy: 0.9896 - val_loss: 0.0321 - val_accuracy: 0.9892\n",
      "Epoch 26/100\n",
      "3200/3200 - 4s - loss: 0.0291 - accuracy: 0.9896 - val_loss: 0.0326 - val_accuracy: 0.9891\n",
      "Epoch 00026: early stopping\n",
      "Training fold 1 completed. macro f1 score : 0.98274\n",
      "Our training dataset shape is (3200, 500, 18)\n",
      "Our validation dataset shape is (800, 500, 18)\n",
      "(3200, 500, 18) (3200, 500, 6)\n",
      "Evaluate on ['2_1' '6_1' '9_1' '9_4']\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/100\n",
      "3200/3200 - 12s - loss: 0.1296 - accuracy: 0.9691 - val_loss: 0.1216 - val_accuracy: 0.9811\n",
      "Epoch 2/100\n",
      "3200/3200 - 4s - loss: 0.0500 - accuracy: 0.9887 - val_loss: 0.0647 - val_accuracy: 0.9831\n",
      "Epoch 3/100\n",
      "3200/3200 - 3s - loss: 0.0424 - accuracy: 0.9894 - val_loss: 0.0544 - val_accuracy: 0.9835\n",
      "Epoch 4/100\n",
      "3200/3200 - 3s - loss: 0.0388 - accuracy: 0.9895 - val_loss: 0.0546 - val_accuracy: 0.9835\n",
      "Epoch 5/100\n",
      "3200/3200 - 4s - loss: 0.0369 - accuracy: 0.9896 - val_loss: 0.0525 - val_accuracy: 0.9835\n",
      "Epoch 6/100\n",
      "3200/3200 - 4s - loss: 0.0375 - accuracy: 0.9895 - val_loss: 0.0542 - val_accuracy: 0.9832\n",
      "Epoch 7/100\n",
      "3200/3200 - 3s - loss: 0.0352 - accuracy: 0.9897 - val_loss: 0.0550 - val_accuracy: 0.9835\n",
      "Epoch 8/100\n",
      "3200/3200 - 3s - loss: 0.0339 - accuracy: 0.9897 - val_loss: 0.0513 - val_accuracy: 0.9835\n",
      "Epoch 9/100\n",
      "3200/3200 - 3s - loss: 0.0335 - accuracy: 0.9897 - val_loss: 0.0528 - val_accuracy: 0.9832\n",
      "Epoch 10/100\n",
      "3200/3200 - 4s - loss: 0.0330 - accuracy: 0.9898 - val_loss: 0.0498 - val_accuracy: 0.9836\n",
      "Epoch 11/100\n",
      "3200/3200 - 4s - loss: 0.0312 - accuracy: 0.9899 - val_loss: 0.0489 - val_accuracy: 0.9837\n",
      "Epoch 12/100\n",
      "3200/3200 - 3s - loss: 0.0314 - accuracy: 0.9899 - val_loss: 0.0491 - val_accuracy: 0.9835\n",
      "Epoch 13/100\n",
      "3200/3200 - 4s - loss: 0.0304 - accuracy: 0.9900 - val_loss: 0.0484 - val_accuracy: 0.9836\n",
      "Epoch 14/100\n",
      "3200/3200 - 3s - loss: 0.0298 - accuracy: 0.9900 - val_loss: 0.0481 - val_accuracy: 0.9836\n",
      "Epoch 15/100\n",
      "3200/3200 - 3s - loss: 0.0295 - accuracy: 0.9901 - val_loss: 0.0480 - val_accuracy: 0.9837\n",
      "Epoch 16/100\n",
      "3200/3200 - 3s - loss: 0.0294 - accuracy: 0.9900 - val_loss: 0.0490 - val_accuracy: 0.9837\n",
      "Epoch 17/100\n",
      "3200/3200 - 3s - loss: 0.0285 - accuracy: 0.9903 - val_loss: 0.0475 - val_accuracy: 0.9838\n",
      "Epoch 18/100\n",
      "3200/3200 - 4s - loss: 0.0287 - accuracy: 0.9903 - val_loss: 0.0512 - val_accuracy: 0.9830\n",
      "Epoch 19/100\n",
      "3200/3200 - 3s - loss: 0.0280 - accuracy: 0.9905 - val_loss: 0.0474 - val_accuracy: 0.9840\n",
      "Epoch 20/100\n",
      "3200/3200 - 3s - loss: 0.0273 - accuracy: 0.9905 - val_loss: 0.0484 - val_accuracy: 0.9839\n",
      "Epoch 21/100\n",
      "3200/3200 - 3s - loss: 0.0264 - accuracy: 0.9908 - val_loss: 0.0466 - val_accuracy: 0.9840\n",
      "Epoch 22/100\n",
      "3200/3200 - 3s - loss: 0.0257 - accuracy: 0.9909 - val_loss: 0.0474 - val_accuracy: 0.9838\n",
      "Epoch 23/100\n",
      "3200/3200 - 3s - loss: 0.0256 - accuracy: 0.9910 - val_loss: 0.0477 - val_accuracy: 0.9837\n",
      "Epoch 24/100\n",
      "3200/3200 - 3s - loss: 0.0252 - accuracy: 0.9911 - val_loss: 0.0477 - val_accuracy: 0.9837\n",
      "Epoch 25/100\n",
      "3200/3200 - 3s - loss: 0.0252 - accuracy: 0.9910 - val_loss: 0.0476 - val_accuracy: 0.9837\n",
      "Epoch 26/100\n",
      "3200/3200 - 3s - loss: 0.0253 - accuracy: 0.9911 - val_loss: 0.0484 - val_accuracy: 0.9837\n",
      "Epoch 00026: early stopping\n",
      "Training fold 2 completed. macro f1 score : 0.98183\n",
      "Our training dataset shape is (3200, 500, 18)\n",
      "Our validation dataset shape is (800, 500, 18)\n",
      "(3200, 500, 18) (3200, 500, 6)\n",
      "Evaluate on ['1_2' '2_2' '6_2' '9_2']\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/100\n",
      "3200/3200 - 12s - loss: 0.1479 - accuracy: 0.9633 - val_loss: 0.2440 - val_accuracy: 0.9678\n",
      "Epoch 2/100\n",
      "3200/3200 - 3s - loss: 0.0549 - accuracy: 0.9876 - val_loss: 0.0427 - val_accuracy: 0.9886\n",
      "Epoch 3/100\n",
      "3200/3200 - 3s - loss: 0.0507 - accuracy: 0.9875 - val_loss: 0.0404 - val_accuracy: 0.9886\n",
      "Epoch 4/100\n",
      "3200/3200 - 4s - loss: 0.0440 - accuracy: 0.9880 - val_loss: 0.0380 - val_accuracy: 0.9885\n",
      "Epoch 5/100\n",
      "3200/3200 - 4s - loss: 0.0422 - accuracy: 0.9881 - val_loss: 0.0370 - val_accuracy: 0.9886\n",
      "Epoch 6/100\n",
      "3200/3200 - 4s - loss: 0.0410 - accuracy: 0.9881 - val_loss: 0.0361 - val_accuracy: 0.9886\n",
      "Epoch 7/100\n",
      "3200/3200 - 3s - loss: 0.0395 - accuracy: 0.9882 - val_loss: 0.0353 - val_accuracy: 0.9886\n",
      "Epoch 8/100\n",
      "3200/3200 - 4s - loss: 0.0387 - accuracy: 0.9882 - val_loss: 0.0344 - val_accuracy: 0.9887\n",
      "Epoch 9/100\n",
      "3200/3200 - 4s - loss: 0.0617 - accuracy: 0.9837 - val_loss: 0.0388 - val_accuracy: 0.9885\n",
      "Epoch 10/100\n",
      "3200/3200 - 4s - loss: 0.0415 - accuracy: 0.9882 - val_loss: 0.0351 - val_accuracy: 0.9887\n",
      "Epoch 11/100\n",
      "3200/3200 - 3s - loss: 0.0388 - accuracy: 0.9884 - val_loss: 0.0345 - val_accuracy: 0.9886\n",
      "Epoch 12/100\n",
      "3200/3200 - 4s - loss: 0.0389 - accuracy: 0.9883 - val_loss: 0.0338 - val_accuracy: 0.9887\n",
      "Epoch 13/100\n",
      "3200/3200 - 4s - loss: 0.0392 - accuracy: 0.9881 - val_loss: 0.0338 - val_accuracy: 0.9888\n",
      "Epoch 14/100\n",
      "3200/3200 - 3s - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.0351 - val_accuracy: 0.9886\n",
      "Epoch 15/100\n",
      "3200/3200 - 4s - loss: 0.0368 - accuracy: 0.9884 - val_loss: 0.0335 - val_accuracy: 0.9888\n",
      "Epoch 16/100\n",
      "3200/3200 - 3s - loss: 0.0363 - accuracy: 0.9884 - val_loss: 0.0345 - val_accuracy: 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "3200/3200 - 3s - loss: 0.0358 - accuracy: 0.9884 - val_loss: 0.0352 - val_accuracy: 0.9885\n",
      "Epoch 18/100\n",
      "3200/3200 - 3s - loss: 0.0357 - accuracy: 0.9885 - val_loss: 0.0329 - val_accuracy: 0.9887\n",
      "Epoch 19/100\n",
      "3200/3200 - 3s - loss: 0.0355 - accuracy: 0.9884 - val_loss: 0.0330 - val_accuracy: 0.9888\n",
      "Epoch 20/100\n",
      "3200/3200 - 3s - loss: 0.0347 - accuracy: 0.9885 - val_loss: 0.0325 - val_accuracy: 0.9886\n",
      "Epoch 21/100\n",
      "3200/3200 - 4s - loss: 0.0341 - accuracy: 0.9885 - val_loss: 0.0325 - val_accuracy: 0.9888\n",
      "Epoch 22/100\n",
      "3200/3200 - 3s - loss: 0.0341 - accuracy: 0.9886 - val_loss: 0.0324 - val_accuracy: 0.9886\n",
      "Epoch 23/100\n",
      "3200/3200 - 3s - loss: 0.0340 - accuracy: 0.9887 - val_loss: 0.0322 - val_accuracy: 0.9888\n",
      "Epoch 24/100\n",
      "3200/3200 - 3s - loss: 0.0336 - accuracy: 0.9886 - val_loss: 0.0324 - val_accuracy: 0.9886\n",
      "Epoch 25/100\n",
      "3200/3200 - 3s - loss: 0.0334 - accuracy: 0.9887 - val_loss: 0.0322 - val_accuracy: 0.9887\n",
      "Epoch 26/100\n",
      "3200/3200 - 3s - loss: 0.0335 - accuracy: 0.9886 - val_loss: 0.0324 - val_accuracy: 0.9887\n",
      "Epoch 27/100\n",
      "3200/3200 - 3s - loss: 0.0332 - accuracy: 0.9887 - val_loss: 0.0322 - val_accuracy: 0.9887\n",
      "Epoch 28/100\n",
      "3200/3200 - 3s - loss: 0.0332 - accuracy: 0.9886 - val_loss: 0.0327 - val_accuracy: 0.9886\n",
      "Epoch 00028: early stopping\n",
      "Training fold 3 completed. macro f1 score : 0.98201\n",
      "Our training dataset shape is (3200, 500, 18)\n",
      "Our validation dataset shape is (800, 500, 18)\n",
      "(3200, 500, 18) (3200, 500, 6)\n",
      "Evaluate on ['1_3' '2_3' '6_3' '9_3']\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/100\n",
      "3200/3200 - 12s - loss: 0.1262 - accuracy: 0.9675 - val_loss: 0.1083 - val_accuracy: 0.9847\n",
      "Epoch 2/100\n",
      "3200/3200 - 3s - loss: 0.0507 - accuracy: 0.9878 - val_loss: 0.0409 - val_accuracy: 0.9884\n",
      "Epoch 3/100\n",
      "3200/3200 - 4s - loss: 0.0526 - accuracy: 0.9869 - val_loss: 0.0417 - val_accuracy: 0.9881\n",
      "Epoch 4/100\n",
      "3200/3200 - 3s - loss: 0.0441 - accuracy: 0.9881 - val_loss: 0.0368 - val_accuracy: 0.9886\n",
      "Epoch 5/100\n",
      "3200/3200 - 4s - loss: 0.0411 - accuracy: 0.9882 - val_loss: 0.0357 - val_accuracy: 0.9885\n",
      "Epoch 6/100\n",
      "3200/3200 - 4s - loss: 0.0405 - accuracy: 0.9882 - val_loss: 0.0382 - val_accuracy: 0.9881\n",
      "Epoch 7/100\n",
      "3200/3200 - 3s - loss: 0.0403 - accuracy: 0.9881 - val_loss: 0.0362 - val_accuracy: 0.9884\n",
      "Epoch 8/100\n",
      "3200/3200 - 3s - loss: 0.0383 - accuracy: 0.9883 - val_loss: 0.0355 - val_accuracy: 0.9885\n",
      "Epoch 9/100\n",
      "3200/3200 - 4s - loss: 0.0524 - accuracy: 0.9859 - val_loss: 0.0418 - val_accuracy: 0.9884\n",
      "Epoch 10/100\n",
      "3200/3200 - 4s - loss: 0.0397 - accuracy: 0.9883 - val_loss: 0.0342 - val_accuracy: 0.9886\n",
      "Epoch 11/100\n",
      "3200/3200 - 3s - loss: 0.0373 - accuracy: 0.9883 - val_loss: 0.0335 - val_accuracy: 0.9886\n",
      "Epoch 12/100\n",
      "3200/3200 - 4s - loss: 0.0373 - accuracy: 0.9883 - val_loss: 0.0338 - val_accuracy: 0.9886\n",
      "Epoch 13/100\n",
      "3200/3200 - 3s - loss: 0.0368 - accuracy: 0.9884 - val_loss: 0.0334 - val_accuracy: 0.9887\n",
      "Epoch 14/100\n",
      "3200/3200 - 3s - loss: 0.0359 - accuracy: 0.9884 - val_loss: 0.0330 - val_accuracy: 0.9887\n",
      "Epoch 15/100\n",
      "3200/3200 - 4s - loss: 0.0355 - accuracy: 0.9885 - val_loss: 0.0328 - val_accuracy: 0.9886\n",
      "Epoch 16/100\n",
      "3200/3200 - 3s - loss: 0.0352 - accuracy: 0.9884 - val_loss: 0.0334 - val_accuracy: 0.9886\n",
      "Epoch 17/100\n",
      "3200/3200 - 3s - loss: 0.0347 - accuracy: 0.9885 - val_loss: 0.0329 - val_accuracy: 0.9886\n",
      "Epoch 18/100\n",
      "3200/3200 - 3s - loss: 0.0349 - accuracy: 0.9885 - val_loss: 0.0324 - val_accuracy: 0.9887\n",
      "Epoch 19/100\n",
      "3200/3200 - 4s - loss: 0.0343 - accuracy: 0.9885 - val_loss: 0.0326 - val_accuracy: 0.9887\n",
      "Epoch 20/100\n",
      "3200/3200 - 4s - loss: 0.0336 - accuracy: 0.9887 - val_loss: 0.0326 - val_accuracy: 0.9887\n",
      "Epoch 21/100\n",
      "3200/3200 - 4s - loss: 0.0328 - accuracy: 0.9888 - val_loss: 0.0322 - val_accuracy: 0.9887\n",
      "Epoch 22/100\n",
      "3200/3200 - 4s - loss: 0.0328 - accuracy: 0.9888 - val_loss: 0.0321 - val_accuracy: 0.9887\n",
      "Epoch 23/100\n",
      "3200/3200 - 3s - loss: 0.0326 - accuracy: 0.9888 - val_loss: 0.0320 - val_accuracy: 0.9887\n",
      "Epoch 24/100\n",
      "3200/3200 - 3s - loss: 0.0323 - accuracy: 0.9888 - val_loss: 0.0322 - val_accuracy: 0.9887\n",
      "Epoch 25/100\n",
      "3200/3200 - 4s - loss: 0.0321 - accuracy: 0.9889 - val_loss: 0.0320 - val_accuracy: 0.9887\n",
      "Epoch 26/100\n",
      "3200/3200 - 4s - loss: 0.0320 - accuracy: 0.9889 - val_loss: 0.0324 - val_accuracy: 0.9887\n",
      "Epoch 27/100\n",
      "3200/3200 - 3s - loss: 0.0319 - accuracy: 0.9889 - val_loss: 0.0326 - val_accuracy: 0.9887\n",
      "Epoch 28/100\n",
      "3200/3200 - 4s - loss: 0.0315 - accuracy: 0.9889 - val_loss: 0.0324 - val_accuracy: 0.9887\n",
      "Epoch 00028: early stopping\n",
      "Training fold 4 completed. macro f1 score : 0.98191\n",
      "Our training dataset shape is (3200, 500, 18)\n",
      "Our validation dataset shape is (800, 500, 18)\n",
      "(3200, 500, 18) (3200, 500, 6)\n",
      "Evaluate on ['1_1' '1_4' '2_4' '6_4']\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/100\n",
      "3200/3200 - 12s - loss: 0.1525 - accuracy: 0.9581 - val_loss: 0.0627 - val_accuracy: 0.9936\n",
      "Epoch 2/100\n",
      "3200/3200 - 4s - loss: 0.0572 - accuracy: 0.9862 - val_loss: 0.0451 - val_accuracy: 0.9936\n",
      "Epoch 3/100\n",
      "3200/3200 - 4s - loss: 0.0673 - accuracy: 0.9832 - val_loss: 0.0236 - val_accuracy: 0.9938\n",
      "Epoch 4/100\n",
      "3200/3200 - 3s - loss: 0.0500 - accuracy: 0.9866 - val_loss: 0.0197 - val_accuracy: 0.9941\n",
      "Epoch 5/100\n",
      "3200/3200 - 3s - loss: 0.0473 - accuracy: 0.9867 - val_loss: 0.0198 - val_accuracy: 0.9941\n",
      "Epoch 6/100\n",
      "3200/3200 - 3s - loss: 0.0462 - accuracy: 0.9867 - val_loss: 0.0196 - val_accuracy: 0.9940\n",
      "Epoch 7/100\n",
      "3200/3200 - 3s - loss: 0.0445 - accuracy: 0.9867 - val_loss: 0.0184 - val_accuracy: 0.9941\n",
      "Epoch 8/100\n",
      "3200/3200 - 3s - loss: 0.0433 - accuracy: 0.9868 - val_loss: 0.0189 - val_accuracy: 0.9941\n",
      "Epoch 9/100\n",
      "3200/3200 - 3s - loss: 0.0437 - accuracy: 0.9868 - val_loss: 0.0183 - val_accuracy: 0.9941\n",
      "Epoch 10/100\n",
      "3200/3200 - 3s - loss: 0.0415 - accuracy: 0.9869 - val_loss: 0.0192 - val_accuracy: 0.9942\n",
      "Epoch 11/100\n",
      "3200/3200 - 3s - loss: 0.0393 - accuracy: 0.9870 - val_loss: 0.0173 - val_accuracy: 0.9942\n",
      "Epoch 12/100\n",
      "3200/3200 - 3s - loss: 0.0391 - accuracy: 0.9871 - val_loss: 0.0173 - val_accuracy: 0.9943\n",
      "Epoch 13/100\n",
      "3200/3200 - 3s - loss: 0.0386 - accuracy: 0.9872 - val_loss: 0.0171 - val_accuracy: 0.9943\n",
      "Epoch 14/100\n",
      "3200/3200 - 3s - loss: 0.0380 - accuracy: 0.9873 - val_loss: 0.0177 - val_accuracy: 0.9942\n",
      "Epoch 15/100\n",
      "3200/3200 - 3s - loss: 0.0377 - accuracy: 0.9873 - val_loss: 0.0173 - val_accuracy: 0.9943\n",
      "Epoch 16/100\n",
      "3200/3200 - 3s - loss: 0.0373 - accuracy: 0.9874 - val_loss: 0.0168 - val_accuracy: 0.9944\n",
      "Epoch 17/100\n",
      "3200/3200 - 4s - loss: 0.0367 - accuracy: 0.9876 - val_loss: 0.0168 - val_accuracy: 0.9944\n",
      "Epoch 18/100\n",
      "3200/3200 - 3s - loss: 0.0368 - accuracy: 0.9876 - val_loss: 0.0171 - val_accuracy: 0.9943\n",
      "Epoch 19/100\n",
      "3200/3200 - 4s - loss: 0.0362 - accuracy: 0.9876 - val_loss: 0.0169 - val_accuracy: 0.9943\n",
      "Epoch 20/100\n",
      "3200/3200 - 3s - loss: 0.0355 - accuracy: 0.9878 - val_loss: 0.0168 - val_accuracy: 0.9943\n",
      "Epoch 21/100\n",
      "3200/3200 - 4s - loss: 0.0343 - accuracy: 0.9880 - val_loss: 0.0166 - val_accuracy: 0.9944\n",
      "Epoch 22/100\n",
      "3200/3200 - 4s - loss: 0.0339 - accuracy: 0.9881 - val_loss: 0.0169 - val_accuracy: 0.9942\n",
      "Epoch 23/100\n",
      "3200/3200 - 4s - loss: 0.0339 - accuracy: 0.9881 - val_loss: 0.0165 - val_accuracy: 0.9944\n",
      "Epoch 24/100\n",
      "3200/3200 - 4s - loss: 0.0337 - accuracy: 0.9882 - val_loss: 0.0165 - val_accuracy: 0.9943\n",
      "Epoch 25/100\n",
      "3200/3200 - 4s - loss: 0.0335 - accuracy: 0.9882 - val_loss: 0.0164 - val_accuracy: 0.9944\n",
      "Epoch 26/100\n",
      "3200/3200 - 4s - loss: 0.0332 - accuracy: 0.9883 - val_loss: 0.0166 - val_accuracy: 0.9942\n",
      "Epoch 27/100\n",
      "3200/3200 - 3s - loss: 0.0330 - accuracy: 0.9884 - val_loss: 0.0169 - val_accuracy: 0.9941\n",
      "Epoch 28/100\n",
      "3200/3200 - 4s - loss: 0.0331 - accuracy: 0.9884 - val_loss: 0.0166 - val_accuracy: 0.9943\n",
      "Epoch 29/100\n",
      "3200/3200 - 4s - loss: 0.0329 - accuracy: 0.9884 - val_loss: 0.0167 - val_accuracy: 0.9942\n",
      "Epoch 30/100\n",
      "3200/3200 - 3s - loss: 0.0327 - accuracy: 0.9885 - val_loss: 0.0165 - val_accuracy: 0.9943\n",
      "Epoch 00030: early stopping\n",
      "Training fold 5 completed. macro f1 score : 0.98340\n"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "for index, (tr_idx, val_idx) in enumerate(gkf.split(df_train, df_train_y, groups)):\n",
    "    train_x, train_y = df_train[tr_idx], df_train_y[tr_idx]\n",
    "    valid_x, valid_y = df_train[val_idx], df_train_y[val_idx]\n",
    "    print(f'Our training dataset shape is {train_x.shape}')\n",
    "    print(f'Our validation dataset shape is {valid_x.shape}')\n",
    "    print(train_x.shape, train_y.shape)\n",
    "    print(\"Evaluate on\", np.unique(groups[val_idx]))\n",
    "    shape_ = (None, train_x.shape[2])\n",
    "    model = Classifier(shape_)\n",
    "    cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    model.fit(train_x,train_y,\n",
    "              epochs = nn_epochs,\n",
    "              callbacks = [cb_lr_schedule, early_stop], #MacroF1(model, valid_x, valid_y) \n",
    "              batch_size = nn_batch_size,verbose = 2,\n",
    "              validation_data = (valid_x,valid_y))\n",
    "    preds_f = model.predict(valid_x)\n",
    "    f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro') \n",
    "    print(f'Training fold {index + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n",
    "    oof_[val_idx] += preds_f\n",
    "    te_preds = model.predict(df_test)\n",
    "    te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
    "    preds_ += te_preds / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T07:24:17.131676Z",
     "start_time": "2020-05-04T07:24:17.125892Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:40:30.939691Z",
     "start_time": "2020-05-04T22:40:30.507549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oof F1 score is 0.9824311667162924\n"
     ]
    }
   ],
   "source": [
    "# goal: NA\n",
    "# for now: 0.9824\n",
    "print(\"oof F1 score is\", f1_score(oof_.reshape([-1,class_num]).argmax(axis=1), df_train_y.reshape([-1,class_num]).argmax(axis=1), average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:40:51.342655Z",
     "start_time": "2020-05-04T22:40:51.314846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oof shape is (2000000,), test pred shape is (100000,)\n"
     ]
    }
   ],
   "source": [
    "oof_pred = oof_.reshape([-1,class_num]).argmax(axis=1)\n",
    "test_pred = preds_.argmax(axis=1)\n",
    "\n",
    "print(f\"oof shape is {oof_pred.shape}, test pred shape is {test_pred.shape}\")\n",
    "# # save oof and prediction\n",
    "# np.save('oof/oof_model_7.npy', oof_pred)\n",
    "# np.save('pred/pred_model_7.npy', test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
