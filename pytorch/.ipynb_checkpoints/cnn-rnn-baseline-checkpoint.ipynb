{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:04.130138Z",
     "start_time": "2020-04-13T23:02:03.250027Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_toolbelt import losses as L\n",
    "from nn_utils import *\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:04.330973Z",
     "start_time": "2020-04-13T23:02:04.131169Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_pickle('../features/train_clean.pkl')\n",
    "df_test_raw = pd.read_pickle('../features/test_clean.pkl')\n",
    "TARGET = \"open_channels\"\n",
    "df_test_raw[TARGET] = 0\n",
    "\n",
    "print(df_train_raw.shape, df_test_raw.shape)\n",
    "df_train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:04.333688Z",
     "start_time": "2020-04-13T23:02:04.332063Z"
    }
   },
   "outputs": [],
   "source": [
    "# # feature engineering here\n",
    "# df_train_raw[\"signal_pow_2\"] = df_train_raw[\"signal\"] ** 2\n",
    "# df_test_raw[\"signal_pow_2\"] = df_test_raw[\"signal\"] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:04.344354Z",
     "start_time": "2020-04-13T23:02:04.334550Z"
    }
   },
   "outputs": [],
   "source": [
    "use_cols = [\n",
    "    col for col in df_train_raw.columns if col not in\n",
    "    [\"time\", \"local_time\", \"open_channels\", \"batch\", \"mini_batch\"]\n",
    "]\n",
    "print(use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:04.354791Z",
     "start_time": "2020-04-13T23:02:04.345221Z"
    }
   },
   "outputs": [],
   "source": [
    "def chop_seq(df_batch_i):\n",
    "\n",
    "    df_batch_i_features = []\n",
    "    df_batch_i_y = []\n",
    "\n",
    "    for i in range(200):\n",
    "\n",
    "        # (2500, 5)\n",
    "        tmp = df_batch_i[(2500 * i):(2500 * (i + 1))]\n",
    "        df_batch_i_features.append(tmp[use_cols].values)\n",
    "        df_batch_i_y.append(tmp[TARGET].values)\n",
    "\n",
    "    return df_batch_i_features, df_batch_i_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:05.491276Z",
     "start_time": "2020-04-13T23:02:04.355715Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "df_train = []\n",
    "df_train_y = []\n",
    "\n",
    "for batch_i in [1, 2, 3, 4, 5, 6, 7, 9, 10]:\n",
    "    df_batch_i = df_train_raw[df_train_raw.batch == batch_i]\n",
    "    df_batch_i_features, df_batch_i_y = chop_seq(df_batch_i)\n",
    "    df_train.append(df_batch_i_features)\n",
    "    df_train_y.append(df_batch_i_y)\n",
    "\n",
    "df_train = np.array(df_train).reshape([-1, 2500, np.array(df_train).shape[-1]]).transpose([0, 2, 1])\n",
    "df_train_y = np.array(df_train_y).reshape([-1, 2500])\n",
    "\n",
    "print(\"TRAIN:\", df_train.shape, df_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:05.969299Z",
     "start_time": "2020-04-13T23:02:05.492149Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "df_test = []\n",
    "df_test_y = []\n",
    "\n",
    "for batch_i in [1, 2, 3, 4]:\n",
    "    df_batch_i = df_test_raw[df_test_raw.batch == batch_i]\n",
    "    df_batch_i_features, df_batch_i_y = chop_seq(df_batch_i)\n",
    "    df_test.append(df_batch_i_features)\n",
    "    df_test_y.append(df_batch_i_y)\n",
    "\n",
    "df_test = np.array(df_test).reshape([-1, 2500, np.array(df_test).shape[-1]]).transpose([0, 2, 1])\n",
    "df_test_y = np.array(df_test_y).reshape([-1, 2500])\n",
    "\n",
    "print(\"TEST:\", df_test.shape, df_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:05.972511Z",
     "start_time": "2020-04-13T23:02:05.970546Z"
    }
   },
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "group = list(range(df_train.shape[0]))\n",
    "cv = 3\n",
    "skf = GroupKFold(n_splits=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:05.983849Z",
     "start_time": "2020-04-13T23:02:05.973279Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqRnn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 seq_len,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 num_layers=1,\n",
    "                 bidirectional=False,\n",
    "                 dropout=0,\n",
    "                 hidden_layers=[100, 200]):\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_size = input_size  # 1\n",
    "        self.seq_len = seq_len  # 2500\n",
    "        self.hidden_size = hidden_size  # 128\n",
    "        self.num_layers = num_layers  # 2\n",
    "        self.bidirectional = bidirectional  # True\n",
    "        self.output_size = output_size  # 11\n",
    "\n",
    "        # CNN\n",
    "        self.cov1 = nn.Conv1d(in_channels=input_size,\n",
    "                              out_channels=256,\n",
    "                              kernel_size=11,\n",
    "                              padding=5)\n",
    "        self.cov2 = nn.Conv1d(in_channels=256,\n",
    "                              out_channels=128,\n",
    "                              kernel_size=11,\n",
    "                              padding=5)\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.GRU(input_size=128,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bidirectional=bidirectional,\n",
    "                          batch_first=True,\n",
    "                          dropout=0.3)\n",
    "        # Input Layer\n",
    "        if hidden_layers and len(hidden_layers):  # [128, 64, 128]\n",
    "            first_layer = nn.Linear(\n",
    "                hidden_size * 2 if bidirectional else hidden_size,  # 128\n",
    "                hidden_layers[0])\n",
    "\n",
    "            # Hidden Layers\n",
    "            self.hidden_layers = nn.ModuleList([first_layer] + [\n",
    "                nn.Linear(hidden_layers[i], hidden_layers[i + 1])\n",
    "                for i in range(len(hidden_layers) - 1)\n",
    "            ])\n",
    "            for layer in self.hidden_layers:\n",
    "                nn.init.kaiming_normal_(layer.weight.data)\n",
    "\n",
    "            self.intermediate_layer = nn.Linear(hidden_layers[-1],\n",
    "                                                self.input_size)\n",
    "            # output layers\n",
    "            self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n",
    "            nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "        else:\n",
    "            self.hidden_layers = []\n",
    "            self.intermediate_layer = nn.Linear(\n",
    "                hidden_size * 2 if bidirectional else hidden_siz,\n",
    "                self.input_size)\n",
    "            self.output_layer = nn.Linear(\n",
    "                hidden_size * 2 if bidirectional else hidden_size, output_size)\n",
    "            nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "        self.activation_fn = torch.relu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = self.cov1(x)\n",
    "        x = self.cov2(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        outputs, hidden = self.rnn(x)\n",
    "\n",
    "        x = self.dropout(self.activation_fn(outputs))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fn(hidden_layer(x))\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:06.022246Z",
     "start_time": "2020-04-13T23:02:05.984778Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = torch.Tensor(df_train)\n",
    "df_test = torch.Tensor(df_test)\n",
    "        \n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T23:02:06.025270Z",
     "start_time": "2020-04-13T23:02:06.023091Z"
    }
   },
   "outputs": [],
   "source": [
    "val_preds_all = np.zeros((df_train_raw.shape[0], 11))\n",
    "test_preds_all = np.zeros((df_test_raw.shape[0], 11))\n",
    "cv_score = np.zeros(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:10:15.736368Z",
     "start_time": "2020-04-14T00:09:55.861791Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./models\"):\n",
    "            os.makedirs(\"./models\")\n",
    "for index, (train_index, val_index) in enumerate(skf.split(df_train, df_train_y, group)):\n",
    "\n",
    "    print(\"Fold \", index, \"TRAIN:\", train_index.shape, \"TEST:\", val_index.shape)\n",
    "    \n",
    "    batchsize = 64\n",
    "    train_dataset = IonDataset(df_train[train_index],  df_train_y[train_index], flip=False, noise_level=0.0, class_split=0.0)\n",
    "    train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "    valid_dataset = IonDataset(df_train[val_index],  df_train_y[val_index], flip=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batchsize, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    test_dataset = IonDataset(df_test,  df_test_y, flip=False, noise_level=0.0, class_split=0.0)\n",
    "    test_dataloader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=16, pin_memory=True)\n",
    "    test_preds_iter = np.zeros((2000000, 11))\n",
    "\n",
    "    for it in range(1):\n",
    "        device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        model=Seq2SeqRnn(input_size=df_train.shape[1], seq_len=2500, hidden_size=128, output_size=11, num_layers=2, hidden_layers=[128,64,32],\n",
    "                         bidirectional=True).to(device)\n",
    "        \n",
    "        no_of_epochs = 200\n",
    "        early_stopping = EarlyStopping(patience=20, is_maximize=True, checkpoint_path=\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it))\n",
    "        criterion = L.FocalLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e2, max_lr=0.001, epochs=no_of_epochs,\n",
    "                                                steps_per_epoch=len(train_dataloader))\n",
    "        avg_train_losses, avg_valid_losses = [], [] \n",
    "    \n",
    "    \n",
    "        for epoch in range(no_of_epochs):\n",
    "            start_time = time.time()\n",
    "    \n",
    "            print(\"Epoch : {}\".format(epoch))\n",
    "            print( \"learning_rate: {:0.9f}\".format(schedular.get_lr()[0]))\n",
    "            train_losses, valid_losses = [], []\n",
    "    \n",
    "            model.train() # prep model for training\n",
    "            train_preds, train_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "    \n",
    "            for x, y in train_dataloader:\n",
    "            \n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(x[:, :df_train.shape[1], :])\n",
    "    \n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "                y_ = y.view(-1)\n",
    "    \n",
    "                loss = criterion(predictions_, y_)\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                schedular.step()\n",
    "                # record training loss\n",
    "                train_losses.append(loss.item())\n",
    "    \n",
    "                train_true = torch.cat([train_true, y_], 0)\n",
    "                train_preds = torch.cat([train_preds, predictions_], 0)\n",
    "\n",
    "            model.eval() # prep model for evaluation\n",
    "            val_preds, val_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "            with torch.no_grad():\n",
    "                for x, y in valid_dataloader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "    \n",
    "#                     predictions = model(x[:,:df_train.shape[1],:])\n",
    "                    predictions = model(x)\n",
    "                    predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "                    y_ = y.view(-1)\n",
    "    \n",
    "                    loss = criterion(predictions_, y_)\n",
    "                    valid_losses.append(loss.item())\n",
    "                    \n",
    "                    val_true = torch.cat([val_true, y_], 0)\n",
    "                    val_preds = torch.cat([val_preds, predictions_], 0)\n",
    "\n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = np.average(train_losses)\n",
    "            valid_loss = np.average(valid_losses)\n",
    "            avg_train_losses.append(train_loss)\n",
    "            avg_valid_losses.append(valid_loss)\n",
    "\n",
    "            print( \"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n",
    "            \n",
    "            train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1), labels=list(range(11)), average='macro')\n",
    "            val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1), labels=list(range(11)), average='macro')\n",
    "            print( \"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n",
    "            \n",
    "            cv_score[index] = val_score\n",
    "            if early_stopping(val_score, model):\n",
    "                print(\"Early Stopping...\")\n",
    "                print(\"Best Val Score: {:0.6f}\".format(early_stopping.best_score))\n",
    "                cv_score[index] = early_stopping.best_score\n",
    "                break\n",
    "            \n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        model.load_state_dict(torch.load(\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it)))\n",
    "        with torch.no_grad():\n",
    "            pred_list = []\n",
    "            for x, y in test_dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                predictions = model(x[:,:df_train.shape[1],:])\n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "\n",
    "                pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy())\n",
    "            test_preds = np.vstack(pred_list)\n",
    "       \n",
    "        test_preds_iter += test_preds\n",
    "        test_preds_all += test_preds\n",
    "        if not os.path.exists(\"./predictions/test\"):\n",
    "            os.makedirs(\"./predictions/test\")\n",
    "        np.save('./predictions/test/gru_clean_fold_{}_iter_{}_raw.npy'.format(index, it), arr=test_preds_iter)\n",
    "        np.save('./predictions/test/gru_clean_fold_{}_raw.npy'.format(index), arr=test_preds_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:01:28.992504Z",
     "start_time": "2020-04-13T23:02:03.276Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"cross validation score is:\", cv_score.mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:01:28.992944Z",
     "start_time": "2020-04-13T23:02:03.278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Best Val Score: \n",
    "sub = pd.read_csv(\"../input/sample_submission.csv\", dtype={'time':str})\n",
    "\n",
    "test_preds_all = test_preds_all/np.sum(test_preds_all, axis=1)[:, None]\n",
    "test_pred_frame = pd.DataFrame({'time': sub['time'].astype(str),\n",
    "                                'open_channels': np.argmax(test_preds_all, axis=1)})\n",
    "# test_pred_frame.to_csv(\"../submissions/cnn_rnn_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
